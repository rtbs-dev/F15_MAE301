{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "You are given the data 'dat_train.txt' which includes a set of cars with their attributes. Your goal is to build a linear model that considers the price of cars (the first column in the data) as the response of the other car attributes. The homework will be graded based on two parts: \n",
    "\n",
    "* (80pt) The following report on model estimation and selection, and \n",
    "* (20pt) the prediction performance of your model using a set of test data. Details will be explained as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "* You will need numpy and matplotlib, as usual, but also the wonderful packages from [Scikit-Learn](http://scikit-learn.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you are using ipython notebook, otherwise comment it out\n",
    "%matplotlib inline \n",
    "\n",
    "# loading all packages... #\n",
    "import numpy as np \n",
    "from sklearn import decomposition\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we've given you several base packages here, but you may need to look at the documentation to find functions better suited to your needs. \n",
    "\n",
    "* Next, you will need to import the data, as an array. \n",
    "    - Make sure the .txt file is in the same directory/folder as your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('dat_train.txt')\n",
    "var=['Price', 'Engine size', \n",
    "      'Cylinders', 'HP', 'City MPG', \n",
    "      'Highway MPG', 'Weight', 'Wheel base', \n",
    "      'Length', 'Width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we talked about Data Preprocessing: The data columns should have zero mean and unit variance. In other words, the data should be centered at the origin and the scales of attributes should not be too different. \n",
    "\n",
    "One way to preprocess is to use an appropriate scaling function in the `sklearn.preprocessing` sub-module. You can find the documentation [here](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing). In the following code, complete the import by erasing the `?` and replacing it with the proper function, after uncommenting the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-1121c39a7822>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-1121c39a7822>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from sklearn.preprocessing import ???\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import ??? #import the standard scaler\n",
    "scaler_func=???() # assign standard scaler to a variable \"scaler_func\"\n",
    "\n",
    "y = data[:,0] #first columns\n",
    "\n",
    "# covariates are the other columns. Dealer cost is not included since it correlates too much with the price #\n",
    "X = data[:,1:] # all the others\n",
    "\n",
    "scaler_func.fit(data) # train the scaler for use later \n",
    "data_scaled = scaler_func.transform(data) # preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Visualize your data (10pt)\n",
    "\n",
    "#### Perform PCA on attributes and plot out all cars on the plan of PC1 and PC2. Tag cars with their prices. (5pt)\n",
    "* Hint: The documentation on PCA can be found [here](http://scikit-learn.org/stable/modules/decomposition.html#decompositions). Like before, find the appropriate function (this time in the `sklearn.decomposition` submodule) and complete the code below. \n",
    "\n",
    "* You may desire to see just how much variance is preserved in 2D space (how \"good\" you approximation is). you can use `explained_variance_ratio_().cumsum()` if you are using PCA or RandomizedPCA, or `svd.explained_variance_ratio_.sum()` if you are using SVD to calculate the *sum* of explained variance in your projection as a fraction of the total (all PCs will be 100%, so 2 PCs should be something lower. Let's put this in the title of our plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import ???\n",
    "\n",
    "pca = ???(n_components=???)\n",
    "X_pca = pca.fit_transform(data_scaled) # Projects data into 2-dimensions, using the two highest PCs\n",
    "\n",
    "#### Leave the plotting code as-is ####\n",
    "from matplotlib.colors import LogNorm # use a logscale for better visualization\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "price = data[:,0] # the first column contains our (unscaled) prices\n",
    "cm = plt.cm.get_cmap('RdYlBu') # what colormap type we want (red->yellow->blue)\n",
    "sc = ax.scatter(X_pca[:,0], X_pca[:,1], \n",
    "                 norm=LogNorm(vmin=price.min(), vmax=price.max()), \n",
    "                 c=price, s=35, cmap=cm) # scatterplot, with color\n",
    "\n",
    "cbar = fig.colorbar(sc, ticks=np.logspace(4., 5.5, num=9)) # add a bar for reference\n",
    "cbar.ax.set_yticklabels(['{:,.0f}'.format(i) for i in np.round(np.logspace(4., 5.5, num=9),-2)])# what bar ticks to have\n",
    "\n",
    "ax.set_title(\"2D PCA, with \"+'{0:.2%}'.format(pca.explained_variance_ratio_.sum())+\" Variance explained\")\n",
    "ax.set_ylabel(\"First PC\")\n",
    "ax.set_xlabel(\"Second PC\")\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the trend/trends we see when we overlay prices? \n",
    "#### What structrure do you see in the Data? What is price's relationship to the PC's? (i.e. Interpret the above plot) (5pt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create a simple linear model (20pt)\n",
    "\n",
    "Like before, find the appropriate function for linear regression (this time in the `sklearn.linear_model` submodule)\n",
    "#### Complete the below code. (5pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ???\n",
    "\n",
    "# response are car prices, i.e., the first column of the data matrix #\n",
    "y_train = data_scaled[:,0]\n",
    "# covariates are the other columns. Dealer cost is not included since it correlates too much with the price #\n",
    "X_train = data_scaled[:,1:]\n",
    "\n",
    "ncv = 10 # number of cross valiation folds *KEEP THIS CONSTANT*\n",
    "\n",
    "lin_reg = ???() #new \"modelling\" object\n",
    "lin_reg.??? # do the regresion on the above\n",
    "yhat = lin_reg.??? #predict prices based on the model\n",
    "\n",
    "\n",
    "# Evaluate the models using crossvalidation\n",
    "scores = cross_validation.cross_val_score(lin_reg,\n",
    "    X_train, y_train, scoring=\"mean_squared_error\", cv=ncv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Leave the plotting code as-is ####\n",
    "from matplotlib import gridspec\n",
    "plt.figure(figsize=(12, 7))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n",
    "\n",
    "ax = plt.subplot(gs[0])\n",
    "plt.scatter(y_train, yhat, label=\"Linear 1deg OLS\", clip_on=False)\n",
    "plt.title(\"CV avg score: {:.3}\".format(abs(scores.mean())))\n",
    "plt.xlabel(\"True car price\")\n",
    "plt.ylabel(\"Predicted Car Price\")\n",
    "plt.xlim((min(y_train), max(y_train)))\n",
    "plt.ylim((min(yhat), max(yhat)))\n",
    "plt.plot(range(100000), label=\"perfect prediction\")\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.subplot(gs[1])\n",
    "ypos = np.arange(len(var[1:]))\n",
    "plt.barh(ypos, lin_reg.coef_, align='center')\n",
    "plt.yticks(ypos, var[1:])\n",
    "ax.yaxis.tick_right()\n",
    "plt.title(\"Coefficient Loadings\")\n",
    "plt.xlim([-1,1])\n",
    "fig.tight_layout(pad=1.9)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the plot (15pt): \n",
    "Is the model reasonable? For what range of prices is the prediction best? What does the CV score mean (should it be large or small)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Now use the same code to create multiple models and compare (30pt)\n",
    "\n",
    "#### Compare models using crossvalidation, AIC and BIC. Discuss how model complexity affects the training error and these model selection criteria. \n",
    "\n",
    "* Here we are making use of a neat API in sklearn called **Pipeline**. This let's us queue up a bunch of tasks and perform all of them, as long as the same target (y) is being used for all of them. We'll tell the pipeline to automatically generate the columns for some desired degree polynomial model, and then fit it using linear OLS.\n",
    "\n",
    "* Once again, fill out the `?` by reading the docs. Use the linear OLS from the last problem, AND find sklearn's function for automatically generating the polynomial columns, in the `sklearn.preprocessing` submodule.\n",
    "\n",
    "#### Complete the code (10pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import ??? # find the module from sklearn.preprocessing that generates polynomial terms\n",
    "\n",
    "\n",
    "# a set of linear models with polynomial terms\n",
    "degrees = [???] # make a list of all degrees of polynomial models to try\n",
    "scores = np.zeros((len(degrees),ncv)) # cv error, for each model type\n",
    "error = np.zeros(len(degrees))\n",
    "\n",
    "fig=plt.figure(figsize=(16, 5))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    \n",
    "    polynomial = ???(degree=degrees[i],\n",
    "                              include_bias=True,\n",
    "                              interaction_only=True)\n",
    "    lin_reg = ???(fit_intercept=False) # our data has been scaled, no need for intercept\n",
    "\n",
    "    pipeline = Pipeline([(\"Polynomial Features\", polynomial),\n",
    "                         (\"Linear OLS\", lin_reg)])\n",
    "    pipeline.??? # do the regresion on the above\n",
    "    yhat = pipeline.??? #predict prices based on the model\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores[i,] = cross_validation.cross_val_score(pipeline,\n",
    "        X_train, y_train, scoring=\"mean_squared_error\", cv=ncv) # add the new CV scores\n",
    "    \n",
    "    # Calculate the training error\n",
    "    error[i] = ((yhat - y)**2).sum()\n",
    "    \n",
    "    #### Leave the plotting code as-is ####\n",
    "    ax = plt.subplot(1, len(degrees), i + 1, sharey=ax)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    plt.scatter(y_train, yhat, label=\"Linear \"+str(i+1)+\"deg OLS\", clip_on=False)\n",
    "    plt.title(\"CV avg score: {:.3}\\nAIC = {:.2e}\\nBIC = {:.2e}\".format(abs(scores[i,].mean()),\n",
    "              2*polynomial.n_output_features_+error[i], np.log(y_train.shape[0])*polynomial.n_output_features_+error[i]))\n",
    "\n",
    "    plt.xlim((min(y_train), max(y_train)))\n",
    "    plt.ylim((min(yhat), max(yhat)))\n",
    "    plt.plot(range(100000), label=\"perfect prediction\")\n",
    "    \n",
    "    plt.xlabel(\"True Car Price\")\n",
    "    plt.ylabel(\"Predicted Car Price\")\n",
    "\n",
    "    plt.legend()\n",
    "fig.suptitle(\"Prediction and \"+str(ncv)+\" Cross-validation for polynomial fits (deg 1-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the results (20 pt):\n",
    "Based on the above, which model seems to be the \"best\"? What is happening as the model complexity increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Propose your own model and compare with the above ones (20pt)\n",
    "\n",
    "#### Propose your own linear model (does not have to contain only polynomial terms). Report its crossvalidation, AIC and BIC scores. \n",
    "\n",
    "* HINT: We suggest taking a look at some of the trends you found in the PCA. Sometimes polynomial transformations of your data are not the best...\n",
    "* ANYTHING in the form $$y = a_1 f_1({\\bf x}) +a_2 f_2({\\bf x}) + \\cdots + a_n f_n({\\bf x})$$ will work fine, but you will need $n$ columns in your matrix that you fit, with each column being the $f_i({\\bf})$ transform. Think about how some of the variables should actually correlate. \n",
    "* If you're struggling to find a good model, try plotting histograms (remember HW1?) of the individual variables. If there are ranges with unusually large counts, you may or may not want to find a function that weights that variable accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now apply your model on the test data \"dat_test.txt\" and report the mean square error.\n",
    "\n",
    "You will get at least 10pt if you correctly complete the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (For curious readers) Example: LASSO \n",
    "Below is another linear regression method. You are not required to know how this works, but we wanted to show you what a slightly more sophisticated regression might look like. Here, we penalize having too many variables by adjusting a parameter $\\alpha$ ($\\alpha$ 0 will return OLS exactly). The \"best\" $\\alpha$ is found via cross-validation. Generally, when we can remove variables that are deemed useless, the prediction will behave better for previously unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "model=LassoCV(cv=ncv, fit_intercept=False) #alpha based on CV for ncv from above\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_train)\n",
    "print model.alpha_\n",
    "\n",
    "scores = cross_validation.cross_val_score(model,\n",
    "        X_train, y_train, scoring=\"mean_squared_error\", cv=ncv)\n",
    "print \"CV avg MSE \",-1*scores.mean()\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "plt.figure(figsize=(12, 7))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n",
    "\n",
    "ax = plt.subplot(gs[0])\n",
    "\n",
    "plt.setp(ax, xticks=(), yticks=())\n",
    "plt.scatter(y_train, yhat, label=\"Linear 1deg LASSO\", clip_on=False)\n",
    "\n",
    "\n",
    "plt.title(\"CV avg score: {:.3}\".format(abs(scores.mean())))\n",
    "plt.xlabel(\"True car price\")\n",
    "plt.ylabel(\"Predicted Car Price\")\n",
    "plt.plot(range(6), label=\"perfect prediction\")\n",
    "#plt.xlim((min(y), max(y)))\n",
    "#plt.ylim((min(yhat), max(yhat)))\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "ax = plt.subplot(gs[1])\n",
    "ypos = np.arange(len(var[1:]))\n",
    "plt.barh(ypos, model.coef_, align='center')\n",
    "plt.yticks(ypos, var[1:])\n",
    "ax.yaxis.tick_right()\n",
    "plt.title(\"Coefficient Loadings\")\n",
    "plt.xlim([-1,1])\n",
    "fig.tight_layout(pad=1.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model.path(X,y)[1]\n",
    "fig, ax=plt.subplots(ncols=2, figsize=(15,7))\n",
    "print -np.log10(model.alpha_)\n",
    "ax[0].plot(-np.log10(model.path(X_train, y_train)[0]), model.path(X_train,y_train)[1].T)\n",
    "ax[0].legend([str(i) for i in range(9)], loc=0)\n",
    "ax[0].axvline(x=-np.log10(model.alpha_), ls='--')\n",
    "ax[0].set_title(\"Coef path for -log10(alpha)\")\n",
    "#ax[0].set_ylim((-5,5))\n",
    "ax[1].plot(-np.log10(model.alphas_), model.mse_path_)\n",
    "ax[1].axvline(x=-np.log10(model.alpha_), ls='--')\n",
    "ax[1].set_title(\"MSE on \"+str(ncv)+\" folds for -log10(alpha)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Validation: \n",
    "This is where things get interesting: I'll import the test set that we've been saving this whole time. Unlike cross-validation, now our model has *never* seen this data, and won't be able to learn from it...we simply are testing it's prediction ability. \n",
    "\n",
    "Note that LASSO performs just slightly better. This is how we will check your model's predictive power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the test data\n",
    "test = np.genfromtxt(\"dat_test.txt\") \n",
    "print test.shape\n",
    "test_scaled = scaler_func.transform(test)\n",
    "Xt = test_scaled[:,1:]\n",
    "#print test[:,1:].mean(axis=0)\n",
    "yt_scale = test_scaled[:,0]\n",
    "yt = test[:,0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "# First the Linear OLS\n",
    "yhat_scale=lin_reg.predict(Xt)\n",
    "yhat = yhat_scale*y.std()+y.mean()\n",
    "\n",
    "ax[0].scatter(yt, yhat)\n",
    "ax[0].plot(range(100000))\n",
    "residual = np.sqrt(np.sum((yt-yhat)**2))\n",
    "ax[0].set_title(\"OLS Residual: {:.3}\".format(residual))\n",
    "\n",
    "#Now the LASSO\n",
    "yhat_scale=model.predict(Xt)\n",
    "yhat = yhat_scale*y.std()+y.mean()\n",
    "\n",
    "\n",
    "ax[1].scatter(yt, yhat)\n",
    "ax[1].plot(range(100000))\n",
    "residual = np.sqrt(np.sum((yt-yhat)**2))\n",
    "ax[1].set_title(\"LASSO Residual: {:.3}\".format(residual))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}